{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR5oiTrm2_9N",
        "outputId": "96952948-70fa-4975-f7b3-d5d3959f8f7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             context  \\\n",
            "0  30대 공무원의 자살 사건에 대한 대화로, 직장 내 스트레스와 사회적 편견에 대한 ...   \n",
            "1  30대 공무원의 자살 사건에 대한 대화로, 직장 내 스트레스와 사회적 편견에 대한 ...   \n",
            "2              30대의 이직과 직장 내 적응에 대한 두려움과 어려움을 나누는 대화   \n",
            "3              30대의 이직과 직장 내 적응에 대한 두려움과 어려움을 나누는 대화   \n",
            "4  5살 어린이가 장기 기증을 통해 친구들을 살리고, 그 과정에서 아버지가 겪는 고통과...   \n",
            "\n",
            "                                            response        label  \\\n",
            "0      공무원의 직무 스트레스와 사회적 편견을 해결하기 위해서 사회적인 노력이 필요해요.  Non-sarcasm   \n",
            "1    공무원이란 직업이 이렇게 스트레스풀어서 많은 사람들이 '꿈의 직장'이라고 하나 봐요.      Sarcasm   \n",
            "2                      이직을 생각할 때마다 적응할 수 있을지 걱정이 돼요.  Non-sarcasm   \n",
            "3         이직이란 정말 가장 쉬운 일이지, 아침에 눈 뜨는 것보다도 어렵지 않다니까!      Sarcasm   \n",
            "4  우리 아이가 친구들을 살리기 위해 정말 큰 결정을 했어요. 그런데 복지 시스템이 더...  Non-sarcasm   \n",
            "\n",
            "                                         explanation  \n",
            "0  Response는 공무원의 직무 스트레스와 사회적 편견을 진지하게 해결하려는 의도를...  \n",
            "1  이 문장은 공무원 직업의 힘든 현실을 풍자적으로 지적하며 '꿈의 직장'이라는 표현이...  \n",
            "2  이직에 대한 걱정을 표현하며 진솔한 감정을 나누고 있기 때문에 Non-sarcasm...  \n",
            "3  이직의 어려움을 강조하기 위해 \"가장 쉬운 일\"이라고 비꼬아 표현함으로써 실제 그 ...  \n",
            "4  이 문장은 아버지가 아이의 결정을 진지하게 존중하며 복지 시스템의 개선을 진심으로 ...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/train_data_merge_0604.csv\"\n",
        "\n",
        "df = pd.read_csv(file_path, encoding='utf-8')\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets\n",
        "!pip install -U transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bps4UQdc8gcq",
        "outputId": "6e03a376-bf3b-42e9-95c1-d34b74eca1d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 라이브러리 로딩\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n"
      ],
      "metadata": {
        "id": "WZnyKUkz8gfE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. NaN 제거\n",
        "df = df.dropna(subset=[\"context\", \"response\", \"label\"])\n",
        "\n",
        "# 2. 라벨 처리 (공백 제거 및 숫자 매핑)\n",
        "df[\"label\"] = df[\"label\"].str.strip().str.capitalize()  # 'non-sarcasm' -> 'Non-sarcasm'\n",
        "df = df[df[\"label\"].isin([\"Sarcasm\", \"Non-sarcasm\"])].copy()\n",
        "df[\"label\"] = df[\"label\"].map({\"Non-sarcasm\": 0, \"Sarcasm\": 1})\n",
        "\n",
        "# 3. 프롬프트 생성\n",
        "df[\"text\"] = df.apply(\n",
        "    lambda row: f\"다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\\n상황: {row['context']}\\n발언: {row['response']}\",\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# 4. 결과 확인\n",
        "print(df[[\"text\", \"label\"]].sample(3).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cccMaqYW8ghi",
        "outputId": "6b7770db-e230-46b8-c6e7-d09da66d509e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                                                       text  label\n",
            "  다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\\n상황: 당근마켓에서의 사기 경험에 대해 이야기하며 인터넷 거래의 위험성을 경고하는 대화\\n발언: 인터넷 거래는 언제나 사기의 위험성이 있으니 주의하는 게 중요해요.      0\n",
            "                        다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\\n상황: 도살장에서 돼지들의 아픔을 느끼고 슬퍼하는 상황\\n발언: 이런 상황에서 돼지들이 겪는 고통을 생각하면 마음이 아프다.      0\n",
            "다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\\n상황: 제품 사용 경험에 대한 부정적인 리뷰\\n발언: 제품 사용이 매우 어려워서 결국 붙이지 않기로 했습니다. 다이소에서 다른 제품을 사는 것이 더 나을 것 같습니다.      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 데이터 분리\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df[\"text\"].tolist(),\n",
        "    df[\"label\"].tolist(),\n",
        "    test_size=0.2,\n",
        "    stratify=df[\"label\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "C59vnjmy_jad"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 토크나이저 및 모델 로딩\n",
        "model_name = \"monologg/kobert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLzLV-wC8gl5",
        "outputId": "dc6b3fda-fcad-40ac-e69b-85aaadb814f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pad_token 보완\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "-AVdGDI_8gnE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Dataset 정의\n",
        "class SarcasmDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_len)\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item"
      ],
      "metadata": {
        "id": "cim0uGHZ8gov"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Dataset 객체 생성\n",
        "train_dataset = SarcasmDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = SarcasmDataset(val_texts, val_labels, tokenizer)"
      ],
      "metadata": {
        "id": "AhNz9nhM_Jm0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Trainer 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    save_strategy=\"no\",  # ✅ 모델 저장 기능 제거\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    report_to=\"wandb\",  # or \"none\"\n",
        "    run_name=\"kobert-kocosa-run\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1tpFPoF8grF",
        "outputId": "b63e5b39-7a35-4d7e-9328-f214e0b7e7c7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-55043a433234>:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fgiInO_d8gtZ",
        "outputId": "710c9327-dc01-4b2f-9083-35a0e2b8b545"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtiger5654\u001b[0m (\u001b[33mtiger5654-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_111315-wyt7ls92</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tiger5654-/huggingface/runs/wyt7ls92' target=\"_blank\">kobert-kocosa-run</a></strong> to <a href='https://wandb.ai/tiger5654-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tiger5654-/huggingface' target=\"_blank\">https://wandb.ai/tiger5654-/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tiger5654-/huggingface/runs/wyt7ls92' target=\"_blank\">https://wandb.ai/tiger5654-/huggingface/runs/wyt7ls92</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [600/600 3:06:24, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.657400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.475600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.345200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.350300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.577800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.402500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.263700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.306100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.338900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.279700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.277300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.324900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.254900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.296300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.268700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.297400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.214500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.252300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.222600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.256700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.185300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.195800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.240900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.291000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.224300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.190000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.348500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.188100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.189500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.209100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.137200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.201500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.147200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.244600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.191000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.125400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.129700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.207500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.191900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.239000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.144500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.109700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.103300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.105400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.117300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.096400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.103200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.215900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.103600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.112500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.146100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.123700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.135100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.095300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.126600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.127100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.104600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.137700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.162400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.173000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=600, training_loss=0.22140011489391326, metrics={'train_runtime': 11246.3968, 'train_samples_per_second': 0.853, 'train_steps_per_second': 0.053, 'total_flos': 631269199572480.0, 'train_loss': 0.22140011489391326, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###########################################\n",
        "# 9-1. 허깅페이스 로그인\n",
        "\n",
        "# WRITE token\n",
        "!huggingface-cli login --token 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 # code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xkib01tO8gyR",
        "outputId": "c688b8e3-da0c-4696-921a-1acfad40cbe5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `skt-BERT` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `skt-BERT`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# KoBERT 토크나이저 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\", trust_remote_code=True)\n",
        "\n",
        "# 저장할 디렉토리 생성\n",
        "save_dir = \"./kobert_sarcasm_tokenizer\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# vocab.txt 복사\n",
        "shutil.copyfile(tokenizer.vocab_file, os.path.join(save_dir, \"vocab.txt\"))\n",
        "\n",
        "# 구성 파일 수동 저장\n",
        "with open(os.path.join(save_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write('{\"do_lower_case\": false, \"unk_token\": \"[UNK]\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\"}')\n",
        "\n",
        "with open(os.path.join(save_dir, \"special_tokens_map.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write('{\"unk_token\": \"[UNK]\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\"}')\n"
      ],
      "metadata": {
        "id": "9KpfdbjZE5SR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=save_dir,\n",
        "    repo_id=\"tlttlto/sktBERT\",\n",
        "    path_in_repo=\"\",  # 루트에 업로드\n",
        "    repo_type=\"model\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "em96hxx_E5V2",
        "outputId": "2e8050f2-db9d-4eb7-d941-c1cfef83d4bc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/tlttlto/sktBERT/commit/3579ed2fcda7cdfd462440b27ad1cbbb8dc7795f', commit_message='Upload folder using huggingface_hub', commit_description='', oid='3579ed2fcda7cdfd462440b27ad1cbbb8dc7795f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tlttlto/sktBERT', endpoint='https://huggingface.co', repo_type='model', repo_id='tlttlto/sktBERT'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(\"monologg/kobert\", trust_remote_code=True)\n",
        "print(tok.vocab_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGBOZJ4kE5XP",
        "outputId": "f751bb89-66aa-4491-bc7b-55899d381389"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.cache/huggingface/hub/models--monologg--kobert/snapshots/38279184ba645e8c94d709fbe92eb5bcb47312c1/tokenizer_78b3253a26.model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "model_file = tok.vocab_file  # .model 파일 경로\n",
        "\n",
        "# 저장 디렉토리\n",
        "save_dir = \"./kobert_sarcasm_tokenizer\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# SentencePiece 모델 복사 (핵심)\n",
        "shutil.copyfile(model_file, os.path.join(save_dir, \"tokenizer_78b3253a26.model\"))\n",
        "\n",
        "# 구성 파일 생성\n",
        "with open(os.path.join(save_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write('{\"tokenizer_class\": \"KoBertTokenizer\"}')\n",
        "\n",
        "with open(os.path.join(save_dir, \"special_tokens_map.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write('{\"unk_token\": \"[UNK]\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\"}')\n"
      ],
      "metadata": {
        "id": "65n-GIfLE5Zu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, login\n",
        "\n",
        "login(\"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\")  # 또는 login(\"hf_...\")\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=save_dir,\n",
        "    repo_id=\"tlttlto/sktBERT\",\n",
        "    path_in_repo=\"\",\n",
        "    repo_type=\"model\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "rQjg91oaE5cM",
        "outputId": "436a0682-e115-4225-bd66-2e6cf734e94f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/tlttlto/sktBERT/commit/586d1caca59a06f521d51c8401b2d3de8a56b598', commit_message='Upload folder using huggingface_hub', commit_description='', oid='586d1caca59a06f521d51c8401b2d3de8a56b598', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tlttlto/sktBERT', endpoint='https://huggingface.co', repo_type='model', repo_id='tlttlto/sktBERT'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import torch\n",
        "\n",
        "# 평가용 Dataset 클래스\n",
        "class SarcasmDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_len)\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item"
      ],
      "metadata": {
        "id": "6Gt41GGAFETX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# 다시 불러오기\n",
        "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\", do_lower_case=False)\n",
        "\n",
        "# 저장\n",
        "tokenizer.save_pretrained(\"./finetuned_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VJ1RLUAFEV6",
        "outputId": "c36840c0-8e85-46e3-88d7-4facbdf5b308"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./finetuned_model/tokenizer_config.json',\n",
              " './finetuned_model/special_tokens_map.json',\n",
              " './finetuned_model/vocab.txt',\n",
              " './finetuned_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./finetuned_model\")\n"
      ],
      "metadata": {
        "id": "pL7pJh2rFEYo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"./finetuned_model\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"./finetuned_model\")\n"
      ],
      "metadata": {
        "id": "PKz8K3dVFEbR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# ✅ 1. 저장된 모델 경로\n",
        "model_path = \"./finetuned_model\"\n",
        "\n",
        "# ✅ 2. 저장된 모델과 토크나이저 불러오기\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br7wkmF-FEdv",
        "outputId": "97ec3b99-b1fe-4915-a102-798aad4294cb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 3. 평가용 데이터셋 클래스 정의\n",
        "class SarcasmDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_len)\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item"
      ],
      "metadata": {
        "id": "BAZ6KFjVFEhh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 4. 평가 데이터셋 구성 (이미 존재하는 val_texts, val_labels 사용)\n",
        "eval_dataset = SarcasmDataset(val_texts, val_labels, tokenizer)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=16)\n"
      ],
      "metadata": {
        "id": "5fAsxoa5E5eu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # 진행률 표시\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# tqdm으로 eval_loader 감싸기\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(eval_loader, desc=\"평가 진행 중\"):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.tolist())\n",
        "        all_labels.extend(labels.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMgetEkmE5iM",
        "outputId": "17393ec3-b599-4ac2-8c1c-c503474fd9f4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "평가 진행 중: 100%|██████████| 50/50 [04:48<00:00,  5.76s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# 평가 결과 출력\n",
        "print(\"분류 리포트:\\n\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7ZBJFj6FTYM",
        "outputId": "4a4ac687-19b6-4e3c-9772-df1e63fd392d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "분류 리포트:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7429    0.5939    0.6601       394\n",
            "           1     0.6701    0.8005    0.7295       406\n",
            "\n",
            "    accuracy                         0.6987       800\n",
            "   macro avg     0.7065    0.6972    0.6948       800\n",
            "weighted avg     0.7059    0.6987    0.6953       800\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "print(\"예측 결과 분포:\", Counter(all_preds))\n",
        "\n",
        "for i in range(20):\n",
        "    print(f\"\\n[예시 {i+1}]\")\n",
        "    print(\"문장:\", val_texts[i])\n",
        "    print(\"실제 라벨:\", val_labels[i])\n",
        "    print(\"예측 라벨:\", all_preds[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4dch9ESFTfs",
        "outputId": "36fac737-870b-45a0-f9ba-f63bd58b9c8b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측 결과 분포: Counter({1: 485, 0: 315})\n",
            "\n",
            "[예시 1]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 아기가 좁쌀베개를 불편해하고 잘 사용하지 못하는 상황\n",
            "발언: 아기를 위해 더 편안한 베개를 찾아보는 것이 좋겠어요.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 0\n",
            "\n",
            "[예시 2]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 게임 자체는 재미있지만, 불필요한 업데이트에 대한 불만을 나타낸 리뷰\n",
            "발언: 게임은 재미있지만, 불필요한 업데이트가 많아 불편합니다.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 0\n",
            "\n",
            "[예시 3]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 여자친구의 늦은 답장 때문에 화가 난 남자의 고민에 대한 대화\n",
            "발언: 요즘 많이 바쁜가 보네. 답장이 늦으면 조금 서운해.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 4]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 글쓴이는 밥 말리와 같은 진정성을 가진 가수가 현대에는 없다고 느끼고 있다. 현대 가수들은 외모와 비주얼에만 집중하고, 진정한 음악적 메시지가 부족하다고 비판하고 있다. 또한, 정치인들도 철학 없이 이익만을 추구하는 모습을 비판하고 있다.\n",
            "발언: 현대 음악 산업은 외적인 요소에 지나치게 의존하고 있어 진정한 예술성과 메시지가 사라지고 있는 것 같습니다.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 5]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 사람들이 뱀을 키우는 것에 대한 두려움과 혐오감을 나누는 대화\n",
            "발언: 뱀을 집에서 키우면 매일매일 서바이벌 게임이겠네요!\n",
            "실제 라벨: 1\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 6]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 버스비 상승으로 인한 경제적 부담과 서울로의 이동 이유에 대한 대화\n",
            "발언: 아, 버스비 오르는 게 마치 서울 여행을 더 특별하게 만들어 주려는 의도인 것 같네요. 보너스로 경제적 부담도 덤으로 주고요!\n",
            "실제 라벨: 1\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 7]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 국민의례를 통해서만 나라사랑을 평가하는 것에 대한 비판과 다양한 방식의 나라사랑 표현을 존중해야 한다는 주장\n",
            "발언: 나라를 사랑하는 방법이 국민의례 말고도 많다고요? 그럼 이제부터는 애국심이란 단어도 사전에서 삭제해야겠네요!\n",
            "실제 라벨: 1\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 8]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 연애 중 기념일 통장이나 커플 통장에 대한 의견과 그에 대한 부정적인 생각을 나누는 대화\n",
            "발언: 기념일 통장은 오히려 서로의 독립성을 침해하고, 자칫하면 불필요한 갈등을 불러올 것 같아.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 9]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 자녀의 태권도 복장을 잘못 가져온 사건으로 인해 부모가 불쾌감을 느끼고 대화하는 상황\n",
            "발언: 다음에는 꼭 맞는 복장을 챙길 수 있도록 더 신경 써야겠어요.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 10]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: A: 휴일에는 어떻게 보내시나요?\n",
            "B: 아무래도 혼자 있는 시간을 좋아해서 주로 혼자 보내곤 합니다.\n",
            "A: 그렇군요. 혹시 특별히 하시는 활동이 있으신가요?\n",
            "B: 저는 주로 커피를 한잔 마시면서 책을 읽는 걸 좋아합니다. \n",
            "발언: A: 와, 당신의 휴일은 정말로 화려하군요.\n",
            "실제 라벨: 1\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 11]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 군대의 급식 상황과 관련된 불만과 논란에 대한 대화\n",
            "발언: 와, 이런 멋진 미쉐린 스타급 요리 덕분에 군입대가 한층 기대되는데요!\n",
            "실제 라벨: 1\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 12]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 고양이를 키우는 것에 대한 어려움과 털 문제, 야행성 특성에 대한 대화\n",
            "발언: 고양이를 키우면서 털과 밤에 활동적인 성격 때문에 관리는 좀 어렵지만, 그래도 정말 사랑스러워요.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 13]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: B: 서울에서 자전거를 이용하시는데 어려움이 있으셨나요?\n",
            "A: 아니요, 따름이가 있어서 편하게 이용하고 있습니다.\n",
            "B: 따름이의 어떤 기능을 가장 유용하게 사용하셨나요?\n",
            "A: 따름이의 GPS 기능이 정말 편리합니다. 하지만 종종 위치정보가 정확하지 않아 조금 불편하기도 합니다.\n",
            "발언: B: 그럼 이제 더 재미있는 경로를 찾아볼 수 있겠네요.\n",
            "실제 라벨: 1\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 14]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 어린 시절 친구를 오랜만에 만났더니 시간이 많이 흘러 서로 나이가 들었음을 깨닫는 상황\n",
            "발언: 정말 오랜만이야! 우리가 이렇게 나이가 들었구나.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 15]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 배송된 상품의 파손에 대한 고객 불만 리뷰\n",
            "발언: 이제 깨진 걸로 조각 맞추기 게임이라도 해야겠네요.\n",
            "실제 라벨: 1\n",
            "예측 라벨: 0\n",
            "\n",
            "[예시 16]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 스트레스가 건강에 악영향을 미치는 상황\n",
            "발언: 네, 스트레스 덕분에 병원비로 경제가 활성화되고 있죠!\n",
            "실제 라벨: 1\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 17]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: A: 베스킨에 갈 생각이신가요?\n",
            "B: 네, 오늘 쿠폰이 있어서 가려고 합니다.\n",
            "A: 좋은 선택이시네요. 어떤 맛을 좋아하시나요?\n",
            "B: 저는 그린 티 맛을 좋아하는데, 요즘은 뉴욕치즈 맛이 안 보이더라고요.\n",
            "A: 아, 그럼 요즘 무슨 맛을 즐겨 드시나요?\n",
            "B: 요즘은 바람과 함께 사라지다는 맛을 먹고 있는데, 이거 한눈팔면 없어져서 항상 먹어야만 해요.\n",
            "발언: A: 그러니까 아이스크림을 지켜야 하는 연인이 있으신 거군요.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 0\n",
            "\n",
            "[예시 18]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 게임의 스토리 길이와 후속 콘텐츠 부족에 대한 불만 리뷰\n",
            "발언: 게임의 스토리가 너무 짧고 후속 콘텐츠가 부족하여 아쉽습니다.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 0\n",
            "\n",
            "[예시 19]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 게임의 아트와 세계관에 대한 긍정적인 시작과 이후의 흥미 감소에 대한 리뷰\n",
            "발언: 처음에는 아트와 세계관이 매력적이었지만, 게임을 진행하면서 점점 흥미를 잃게 되었습니다.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 0\n",
            "\n",
            "[예시 20]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 주말 점심 시간에 영화관이 거의 비어 있는 상황\n",
            "발언: 이 시간대에 영화관이 한산해서 편안하게 영화를 볼 수 있겠네요.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xi1VyObNFTi_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hl4q81oSFTmC"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}