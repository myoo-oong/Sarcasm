{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-N-gs91t8Eok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR5oiTrm2_9N",
        "outputId": "6d26d73a-2b14-45c6-8e7d-41c1bc361f0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             context  \\\n",
            "0  30대 공무원의 자살 사건에 대한 대화로, 직장 내 스트레스와 사회적 편견에 대한 ...   \n",
            "1  30대 공무원의 자살 사건에 대한 대화로, 직장 내 스트레스와 사회적 편견에 대한 ...   \n",
            "2              30대의 이직과 직장 내 적응에 대한 두려움과 어려움을 나누는 대화   \n",
            "3              30대의 이직과 직장 내 적응에 대한 두려움과 어려움을 나누는 대화   \n",
            "4  5살 어린이가 장기 기증을 통해 친구들을 살리고, 그 과정에서 아버지가 겪는 고통과...   \n",
            "\n",
            "                                            response        label  \\\n",
            "0      공무원의 직무 스트레스와 사회적 편견을 해결하기 위해서 사회적인 노력이 필요해요.  Non-sarcasm   \n",
            "1    공무원이란 직업이 이렇게 스트레스풀어서 많은 사람들이 '꿈의 직장'이라고 하나 봐요.      Sarcasm   \n",
            "2                      이직을 생각할 때마다 적응할 수 있을지 걱정이 돼요.  Non-sarcasm   \n",
            "3         이직이란 정말 가장 쉬운 일이지, 아침에 눈 뜨는 것보다도 어렵지 않다니까!      Sarcasm   \n",
            "4  우리 아이가 친구들을 살리기 위해 정말 큰 결정을 했어요. 그런데 복지 시스템이 더...  Non-sarcasm   \n",
            "\n",
            "                                         explanation  \n",
            "0  Response는 공무원의 직무 스트레스와 사회적 편견을 진지하게 해결하려는 의도를...  \n",
            "1  이 문장은 공무원 직업의 힘든 현실을 풍자적으로 지적하며 '꿈의 직장'이라는 표현이...  \n",
            "2  이직에 대한 걱정을 표현하며 진솔한 감정을 나누고 있기 때문에 Non-sarcasm...  \n",
            "3  이직의 어려움을 강조하기 위해 \"가장 쉬운 일\"이라고 비꼬아 표현함으로써 실제 그 ...  \n",
            "4  이 문장은 아버지가 아이의 결정을 진지하게 존중하며 복지 시스템의 개선을 진심으로 ...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/train_data_merge_0604.csv\"\n",
        "\n",
        "df = pd.read_csv(file_path, encoding='utf-8')\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets\n",
        "!pip install -U transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bps4UQdc8gcq",
        "outputId": "8804ab18-d2fb-4830-cea4-a0e2e07f48ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 라이브러리 로딩\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n"
      ],
      "metadata": {
        "id": "WZnyKUkz8gfE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. NaN 제거\n",
        "df = df.dropna(subset=[\"context\", \"response\", \"label\"])\n",
        "\n",
        "# 2. 라벨 처리 (공백 제거 및 숫자 매핑)\n",
        "df[\"label\"] = df[\"label\"].str.strip().str.capitalize()  # 'non-sarcasm' -> 'Non-sarcasm'\n",
        "df = df[df[\"label\"].isin([\"Sarcasm\", \"Non-sarcasm\"])].copy()\n",
        "df[\"label\"] = df[\"label\"].map({\"Non-sarcasm\": 0, \"Sarcasm\": 1})\n",
        "\n",
        "# 3. 프롬프트 생성\n",
        "df[\"text\"] = df.apply(\n",
        "    lambda row: f\"다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\\n상황: {row['context']}\\n발언: {row['response']}\",\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# 4. 결과 확인\n",
        "print(df[[\"text\", \"label\"]].sample(3).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cccMaqYW8ghi",
        "outputId": "abfb367a-ed56-4fcb-c22e-d6e00e757d28"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                                                                  text  label\n",
            "                                          다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\\n상황: 상품의 가격에 대한 고객 불만 리뷰\\n발언: 여긴 다이소보다 고급진 가격을 자랑하네요, 거의 명품 수준!      1\n",
            "다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\\n상황: 전라남도 목포와 제주도를 해저터널로 연결하는 계획에 대한 논의와 그에 따른 수요 및 경제성에 대한 우려\\n발언: 목포와 제주도를 연결하는 해저터널이 경제성이 있을지 잘 모르겠어요.      0\n",
            "                    다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\\n상황: 게임 회사 NC의 게임에 실망한 상황\\n발언: 아마도 그 게임들은 중독성을 넘어서 마법의 게임인가 봐요. 빠져나오지 못하게 하는 마법 말이에요.      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 데이터 분리\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df[\"text\"].tolist(),\n",
        "    df[\"label\"].tolist(),\n",
        "    test_size=0.2,\n",
        "    stratify=df[\"label\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "C59vnjmy_jad"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 토크나이저 및 모델 로딩\n",
        "model_name = \"monologg/kobert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLzLV-wC8gl5",
        "outputId": "3547ce42-9db4-46d9-e966-2a0b277d6b62"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pad_token 보완\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "-AVdGDI_8gnE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Dataset 정의\n",
        "class SarcasmDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_len)\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item"
      ],
      "metadata": {
        "id": "cim0uGHZ8gov"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Dataset 객체 생성\n",
        "train_dataset = SarcasmDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = SarcasmDataset(val_texts, val_labels, tokenizer)"
      ],
      "metadata": {
        "id": "AhNz9nhM_Jm0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Trainer 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    save_strategy=\"no\",  # ✅ 모델 저장 기능 제거\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    report_to=\"wandb\",  # or \"none\"\n",
        "    run_name=\"kobert-kocosa-run\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1tpFPoF8grF",
        "outputId": "03a3f96f-d869-49ca-e87a-590646841d9d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-fcbd366bded6>:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "fgiInO_d8gtZ",
        "outputId": "323a07af-6e85-4636-d6da-d5608e159e80"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtiger5654\u001b[0m (\u001b[33mtiger5654-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_092809-cjuryxh7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tiger5654-/huggingface/runs/cjuryxh7' target=\"_blank\">kobert-kocosa-run</a></strong> to <a href='https://wandb.ai/tiger5654-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tiger5654-/huggingface' target=\"_blank\">https://wandb.ai/tiger5654-/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tiger5654-/huggingface/runs/cjuryxh7' target=\"_blank\">https://wandb.ai/tiger5654-/huggingface/runs/cjuryxh7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 1:03:22, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.736700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.690700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.631400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.609800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.490300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.393400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.315700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.354600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.418700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.255600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.204700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.282800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.321100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.306400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.250100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.247100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.285400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.196500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.220100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.218800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=200, training_loss=0.371493718624115, metrics={'train_runtime': 3847.1668, 'train_samples_per_second': 0.832, 'train_steps_per_second': 0.052, 'total_flos': 210423066524160.0, 'train_loss': 0.371493718624115, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###########################################\n",
        "# 9-1. 허깅페이스 로그인\n",
        "\n",
        "# WRITE token\n",
        "!huggingface-cli login --token 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 # code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xkib01tO8gyR",
        "outputId": "2ca22de1-a246-4708-f09b-e6ce6c4251a8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `skt-BERT` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `skt-BERT`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# KoBERT 토크나이저 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\", trust_remote_code=True)\n",
        "\n",
        "# 저장할 디렉토리 생성\n",
        "save_dir = \"./kobert_sarcasm_tokenizer\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# vocab.txt 복사\n",
        "shutil.copyfile(tokenizer.vocab_file, os.path.join(save_dir, \"vocab.txt\"))\n",
        "\n",
        "# 구성 파일 수동 저장\n",
        "with open(os.path.join(save_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write('{\"do_lower_case\": false, \"unk_token\": \"[UNK]\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\"}')\n",
        "\n",
        "with open(os.path.join(save_dir, \"special_tokens_map.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write('{\"unk_token\": \"[UNK]\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\"}')\n"
      ],
      "metadata": {
        "id": "9KpfdbjZE5SR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=save_dir,\n",
        "    repo_id=\"tlttlto/sktBERT\",\n",
        "    path_in_repo=\"\",  # 루트에 업로드\n",
        "    repo_type=\"model\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "em96hxx_E5V2",
        "outputId": "3d03970a-08e0-4c93-acc3-45682bcc51f4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/tlttlto/sktBERT/commit/249fd8aca5e261ec8cd2c848a6ace06d59ae6aff', commit_message='Upload folder using huggingface_hub', commit_description='', oid='249fd8aca5e261ec8cd2c848a6ace06d59ae6aff', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tlttlto/sktBERT', endpoint='https://huggingface.co', repo_type='model', repo_id='tlttlto/sktBERT'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(\"monologg/kobert\", trust_remote_code=True)\n",
        "print(tok.vocab_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGBOZJ4kE5XP",
        "outputId": "606afb43-716f-421d-c55c-dd3e257e6d4a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.cache/huggingface/hub/models--monologg--kobert/snapshots/38279184ba645e8c94d709fbe92eb5bcb47312c1/tokenizer_78b3253a26.model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "model_file = tok.vocab_file  # .model 파일 경로\n",
        "\n",
        "# 저장 디렉토리\n",
        "save_dir = \"./kobert_sarcasm_tokenizer\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# SentencePiece 모델 복사 (핵심)\n",
        "shutil.copyfile(model_file, os.path.join(save_dir, \"tokenizer_78b3253a26.model\"))\n",
        "\n",
        "# 구성 파일 생성\n",
        "with open(os.path.join(save_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write('{\"tokenizer_class\": \"KoBertTokenizer\"}')\n",
        "\n",
        "with open(os.path.join(save_dir, \"special_tokens_map.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write('{\"unk_token\": \"[UNK]\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\"}')\n"
      ],
      "metadata": {
        "id": "65n-GIfLE5Zu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, login\n",
        "\n",
        "login(\"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\")  # 또는 login(\"hf_...\")\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=save_dir,\n",
        "    repo_id=\"tlttlto/sktBERT\",\n",
        "    path_in_repo=\"\",\n",
        "    repo_type=\"model\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "rQjg91oaE5cM",
        "outputId": "aef601a9-2379-4143-a7cf-c9554e695364"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/tlttlto/sktBERT/commit/90a47c18887cec940200a6dec37c9ee60c97932a', commit_message='Upload folder using huggingface_hub', commit_description='', oid='90a47c18887cec940200a6dec37c9ee60c97932a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tlttlto/sktBERT', endpoint='https://huggingface.co', repo_type='model', repo_id='tlttlto/sktBERT'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import torch\n",
        "\n",
        "# 평가용 Dataset 클래스\n",
        "class SarcasmDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_len)\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item"
      ],
      "metadata": {
        "id": "6Gt41GGAFETX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# 다시 불러오기\n",
        "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\", do_lower_case=False)\n",
        "\n",
        "# 저장\n",
        "tokenizer.save_pretrained(\"./finetuned_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VJ1RLUAFEV6",
        "outputId": "d325ef6a-85bb-45d3-c64b-6b3bd9465699"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./finetuned_model/tokenizer_config.json',\n",
              " './finetuned_model/special_tokens_map.json',\n",
              " './finetuned_model/vocab.txt',\n",
              " './finetuned_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./finetuned_model\")\n"
      ],
      "metadata": {
        "id": "pL7pJh2rFEYo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"./finetuned_model\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"./finetuned_model\")\n"
      ],
      "metadata": {
        "id": "PKz8K3dVFEbR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# ✅ 1. 저장된 모델 경로\n",
        "model_path = \"./finetuned_model\"\n",
        "\n",
        "# ✅ 2. 저장된 모델과 토크나이저 불러오기\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br7wkmF-FEdv",
        "outputId": "1154799a-852e-47c6-e7c1-7fd044aef799"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 3. 평가용 데이터셋 클래스 정의\n",
        "class SarcasmDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_len)\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item"
      ],
      "metadata": {
        "id": "BAZ6KFjVFEhh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 4. 평가 데이터셋 구성 (이미 존재하는 val_texts, val_labels 사용)\n",
        "eval_dataset = SarcasmDataset(val_texts, val_labels, tokenizer)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=16)\n"
      ],
      "metadata": {
        "id": "5fAsxoa5E5eu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # 진행률 표시\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# tqdm으로 eval_loader 감싸기\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(eval_loader, desc=\"평가 진행 중\"):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.tolist())\n",
        "        all_labels.extend(labels.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMgetEkmE5iM",
        "outputId": "27fb2976-a3fc-4ada-d3ac-5f1fb859b3ee"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "평가 진행 중: 100%|██████████| 50/50 [04:49<00:00,  5.80s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# 평가 결과 출력\n",
        "print(\"분류 리포트:\\n\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7ZBJFj6FTYM",
        "outputId": "e9dd9ea2-79a1-4752-ec5a-e34efcd039d3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "분류 리포트:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8166    0.8249    0.8207       394\n",
            "           1     0.8284    0.8202    0.8243       406\n",
            "\n",
            "    accuracy                         0.8225       800\n",
            "   macro avg     0.8225    0.8225    0.8225       800\n",
            "weighted avg     0.8226    0.8225    0.8225       800\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "print(\"예측 결과 분포:\", Counter(all_preds))\n",
        "\n",
        "for i in range(20):\n",
        "    print(f\"\\n[예시 {i+1}]\")\n",
        "    print(\"문장:\", val_texts[i])\n",
        "    print(\"실제 라벨:\", val_labels[i])\n",
        "    print(\"예측 라벨:\", all_preds[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4dch9ESFTfs",
        "outputId": "90ddc569-3768-4637-d759-a83c3c143bce"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측 결과 분포: Counter({1: 402, 0: 398})\n",
            "\n",
            "[예시 1]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 군대의 식사 개선에 대한 논의와 중국 재료 사용에 대한 우려가 담긴 대화\n",
            "발언: 군대 식사 개선에 중국 재료가 들어간다니 좀 우려가 되네요.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 0\n",
            "\n",
            "[예시 2]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: B: 너 오늘 플랭크 도전해 볼래? \n",
            "A: 플랭크? 그게 뭐야?\n",
            "B: 바닥에 엎드려서 팔로 몸을 지탱하는 운동이야. \n",
            "A: 응 그럼 우리 그것도 하고 조깅도 같이 해볼까?\n",
            "발언: B: 응 좋아, 너 플랭크 먼저 시작해 봐. 나는 조깅하면서 기다릴게.\n",
            "실제 라벨: 1\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 3]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 누군가의 행동이나 발언이 어리석거나 부적절하여 안타까움을 느끼는 상황\n",
            "발언: 그 사람이 그렇게 행동한 이유가 궁금하지만, 정말 안타깝네요.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 0\n",
            "\n",
            "[예시 4]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 영화나 드라마를 기대하고 봤는데 일상적인 내용만 보여주는 상황\n",
            "발언: 이 영화는 일상적인 삶의 소소한 순간들을 잘 담아내고 있어서 공감이 갔어요.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 0\n",
            "\n",
            "[예시 5]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: A: 매일 뭘 먹을지 고민하시나요?\n",
            "B: 네, 매일 그런 고민을 하게 됩니다.\n",
            "A: 저는 주로 계획적으로 식사를 하려고 합니다.\n",
            "B: 아, 저는 그게 너무 어렵더라고요. 그래서 매일 뭘 먹을지 고민하게 됩니다.\n",
            "발언: A: 아, 저는 그런 걱정 없이 살 수 없어서 정말 부럽습니다.\n",
            "실제 라벨: 1\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 6]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 갑자기 추워진 날씨와 환절기 감기에 대한 우려를 나눈 대화\n",
            "발언: 와, 날씨가 이렇게 빨리 추워질 줄이야! 이번 감기 시즌은 정말 기대됩니다!\n",
            "실제 라벨: 1\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 7]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 게임의 그래픽과 콘텐츠에 대한 실망감 표현\n",
            "발언: 그래픽은 좋지만, 콘텐츠가 빈약하고 전투가 재미없습니다.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 0\n",
            "\n",
            "[예시 8]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 배송 지연과 제품의 냄새에 대한 혼합된 감정의 리뷰\n",
            "발언: 배송이 오래 걸려 기분이 좋지 않았지만, 제품이 잘 도착해서 다행입니다. 다만, 강한 냄새가 나서 아쉽습니다.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 0\n",
            "\n",
            "[예시 9]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 손톱이 자주 부러지는 문제에 대해 영양 부족과 네일 관리의 영향을 논의하며 건강한 손톱의 중요성을 이야기하는 대화\n",
            "발언: 손톱이 자주 부러져서 정말 속상해. 영양제랑 네일 관리를 더 신경 써야겠어.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 0\n",
            "\n",
            "[예시 10]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: B: 어제 맛있는 녀석들 보고 냉장고를 부탁해도 봤어?\n",
            "A: 아니 그럼 시간이 어디서 나겠어? 그런 건 별로 안 봐.\n",
            "B: 신서유기에 이승기 나오던데, 너 좋아하잖아?\n",
            "A: 맞아, 이승기 정말 좋아하는데 그런 예능 프로그램은 시간 낭비 같아서 안 봐.\n",
            "발언: B: 이승기를 진짜 잘 알겠다, 그렇게 보지도 않으면서.\n",
            "실제 라벨: 1\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 11]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 대면 수업으로 전환되면서 피곤함과 과제에 대한 회상과 불만을 나누는 대화\n",
            "발언: 아, 대면 수업 정말 그리웠죠! 과제 때문에 잠은 줄었지만, 교실에서 쓰러질 준비는 항상 되어있어요!\n",
            "실제 라벨: 1\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 12]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 게임의 조작감과 타격감에 대한 부정적인 리뷰\n",
            "발언: 조작감이 좋지 않고, 타격감도 만족스럽지 않습니다.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 0\n",
            "\n",
            "[예시 13]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 어떤 사람이 자신의 의견이나 이야기를 너무 길게 하거나 자주 해서 듣는 사람이 지친 상황\n",
            "발언: 이제 그분의 이야기는 베스트셀러 작가의 데뷔작으로 출간될 예정입니다. 기대 만발!\n",
            "실제 라벨: 1\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 14]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 제품의 품질 문제로 인한 불만 리뷰\n",
            "발언: 제품이 금방 고장 나서 돈을 더 주고 다른 제품을 사는 것이 낫습니다.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 0\n",
            "\n",
            "[예시 15]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 정치적 또는 사회적 상황에서 누군가가 개선을 약속하지만, 실제로는 변화가 없거나 부정적인 결과가 계속되는 상황\n",
            "발언: 아, 이제는 정말 잘해보겠다는 그 약속이 몇 번째인지 기억도 안 나네요. 이번에는 정말로 국민들이 편히 살 수 있게 해주겠죠? 물론, 그 전에 또 다른 핑계가 나오지 않는다면요.\n",
            "실제 라벨: 1\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 16]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: B: 동물을 키우는 것에 대해 어떻게 생각하십니까?\n",
            "A: 사실, 저는 일을 너무 많이 해서 그런 책임을 다하지 못할 것 같습니다.\n",
            "B: 저도 마찬가지입니다. 저희 같은 경우에는 키울 시간이 부족하다는 문제가 있습니다.\n",
            "발언: A: 정확히 그렇습니다. 그런 경험은 한정적일 수밖에 없습니다.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 17]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 소년 촉법의 폐지 필요성과 청소년 범죄 증가에 대한 우려를 논의하는 대화\n",
            "발언: 아, 그래요? 청소년 범죄가 늘어나는 건 그냥 성장의 한 단계인가 보네요, 아무 걱정 없이 지켜봐야겠어요.\n",
            "실제 라벨: 1\n",
            "예측 라벨: 1\n",
            "\n",
            "[예시 18]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 정부가 다주택자에게 높은 세금을 부과하고 있으며, 이에 대한 불만이 존재하는 상황\n",
            "발언: 다주택자들이 높은 세금으로 인해 재정적 어려움을 겪고 있다는 주장은 충분히 이해할 수 있습니다.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 0\n",
            "\n",
            "[예시 19]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 세월호 사고를 떠올리며, 비슷한 상황이나 대응 방식에 대한 불만이나 실망을 표현하는 상황\n",
            "발언: 우리는 과거의 비극에서 교훈을 얻어야 하며, 더 나은 안전 대책을 마련해야 합니다.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 0\n",
            "\n",
            "[예시 20]\n",
            "문장: 다음 상황을 읽고, 이어지는 발언이 풍자(Sarcasm)인지 아닌지 분류하세요.\n",
            "상황: 구매한 제품의 사이즈가 커서 사용에 불편함을 겪고 있는 고객 리뷰\n",
            "발언: 제품의 사이즈가 너무 커서 안전벨트를 사용할 때 끝까지 들어가지 않아 불편합니다. 작은 사이즈를 살 걸 그랬어요.\n",
            "실제 라벨: 0\n",
            "예측 라벨: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xi1VyObNFTi_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hl4q81oSFTmC"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}